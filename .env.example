# LLM Configuration
LLM_MODEL=llama3.2
# Available models with tool support: llama3.1, llama3.2, qwen2.5, etc.
# Make sure to pull the model first: docker exec ollama ollama pull <model_name>

# Backend Configuration
OLLAMA_URL=http://ollama:11434
CORS_ORIGINS=http://localhost:3000,http://localhost:80

# Production Configuration (for Traefik)
# TRAEFIK_HOST=your-domain.com