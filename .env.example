# LLM Configuration
LLM_MODEL=llama2
# Available models: llama2, codellama, mistral, neural-chat, etc.
# Make sure to pull the model first: docker exec ollama ollama pull <model_name>

# Backend Configuration
OLLAMA_URL=http://ollama:11434
CORS_ORIGINS=http://localhost:3000,http://localhost:80

# Production Configuration (for Traefik)
# TRAEFIK_HOST=your-domain.com