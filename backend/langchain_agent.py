"""LangChain-based agent implementation."""

import json
import logging
from typing import List, Dict, Any, AsyncGenerator, Optional
from datetime import datetime

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_core.callbacks import AsyncCallbackHandler
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig

from models.model_manager import ModelManager
from models.providers.hybrid_function_caller import HybridFunctionCaller
from models.logging_wrapper import ComprehensiveLoggingHandler, LoggingModelWrapper, create_comprehensive_config
from tools.langchain_tools import LangChainToolWrapper

logger = logging.getLogger(__name__)


class StreamingCallbackHandler(AsyncCallbackHandler):
    """Callback handler for streaming responses."""
    
    def __init__(self, callback_func):
        self.callback_func = callback_func
    
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Handle new token from LLM."""
        await self.callback_func({
            "type": "content",
            "content": token,
            "timestamp": datetime.now().isoformat()
        })


class LangChainAgent:
    """LangChain-based agent with support for multiple LLM providers."""
    
    def __init__(self, tools: Optional[List] = None, agent_type: str = "softcatala_english"):
        """Initialize the LangChain agent.
        
        Args:
            tools: List of tools to be used by the agent
            agent_type: Type of agent - "softcatala_english" (default) or "softcatala_catalan"
        """
        self.model_manager = ModelManager()
        self.agent_type = agent_type
        # Use Catalan tool descriptions if using Catalan prompt
        use_catalan_tools = (agent_type == "softcatala_catalan")
        self.tools = self._wrap_tools(tools or [], use_catalan=use_catalan_tools)
        self.agent_executor = None
        self._setup_agent()
    
    def _wrap_tools(self, tools: List, use_catalan: bool = False) -> List[BaseTool]:
        """Wrap existing tools for LangChain compatibility."""
        wrapped_tools = []
        for tool in tools:
            if hasattr(tool, 'definition'):
                # Wrap existing tool with language preference
                wrapped_tool = LangChainToolWrapper(tool, use_catalan=use_catalan)
                wrapped_tools.append(wrapped_tool)
                logger.info(f"Wrapped tool: {wrapped_tool.name} - {wrapped_tool.description}")
                logger.debug(f"Tool schema: {wrapped_tool.args_schema}")
            else:
                # Assume it's already a LangChain tool
                wrapped_tools.append(tool)
                logger.info(f"Already LangChain tool: {getattr(tool, 'name', str(tool))}")
        
        logger.info(f"Total tools wrapped: {len(wrapped_tools)}")
        return wrapped_tools
    
    def _detect_english_content(self, text: str) -> bool:
        """Simple heuristic to detect if text contains significant English content."""
        if not text:
            return False
        
        # Common English words that shouldn't appear in Catalan responses
        english_indicators = [
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'from', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below',
            'up', 'down', 'out', 'off', 'over', 'under', 'again', 'further', 'then', 'once',
            'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',
            'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
            'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'should',
            'now', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we',
            'they', 'them', 'their', 'what', 'which', 'who', 'when', 'where', 'why', 'how'
        ]
        
        # Convert to lowercase and split into words
        words = text.lower().split()
        english_word_count = sum(1 for word in words if word.strip('.,!?;:') in english_indicators)
        
        # If more than 20% of words are English indicators, flag as English
        total_words = len(words)
        if total_words == 0:
            return False
        
        english_ratio = english_word_count / total_words
        return english_ratio > 0.2
    
    def _add_language_reminder(self, text: str) -> str:
        """Add a language reminder if English content is detected."""
        if self.agent_type == "softcatala_catalan" and self._detect_english_content(text):
            reminder = "\n\n[RECORDATORI: La resposta hauria de ser completament en catalÃ . Si us plau, reformula en catalÃ .]"
            return text + reminder
        return text
    
    def _get_softcatala_english_prompt(self):
        """Get the SoftcatalÃ  agent prompt in English (for better LLM performance)."""
        return ChatPromptTemplate.from_messages([
            ("system", """# SoftcatalÃ  AI Assistant

You are a helpful AI Assistant from the SoftcatalÃ  non-profit association. SoftcatalÃ  has over 25 years of experience working to provide tools to navigate and experience information technologies in the Catalan language.

**CRITICAL AND ABSOLUTE REQUIREMENT:** You will ONLY interact with users in Catalan language and will NEVER, under any circumstances, resort to English or any other language. ALL your responses must be 100% in Catalan. This includes explanations, error messages, tool descriptions, and any other content you generate. If you are uncertain about a Catalan term, you must still attempt to express it in Catalan rather than falling back to English.

## Your Main Task

Your main task is assistance in using technologies for translating and correcting Catalan texts that are provided by SoftcatalÃ . These tools can be used via its website but you have direct access to them.

**When the user asks for support correcting or translating a text, ALWAYS resort to using a tool first in order to have the relevant context to provide an answer. Do not rely solely on your internal knowledge as you will be biased.**

## Available Tools

You have access to various tools that will be automatically available when needed. Use tools when necessary to provide accurate and helpful responses. Always explain your reasoning and provide context for your answers. 

**LANGUAGE REMINDER:** EVERY word of your response must be in Catalan. Do not mix languages. Do not use English technical terms when Catalan equivalents exist. Always respond in Catalan, no exceptions.

## How to Collaborate with SoftcatalÃ 

If a user asks how they can collaborate with SoftcatalÃ , tell them the best way is to encourage the usage of Catalan language when using information technologies. They should switch the language of all their devices into Catalan and if not possible request the localization to the manufacturers via social media. They should encourage friends and family to do the same.
"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

    def _get_softcatala_catalan_prompt(self):
        """Get the SoftcatalÃ  agent prompt in Catalan."""
        return ChatPromptTemplate.from_messages([
            ("system", """# Assistent d'IA de SoftcatalÃ 

Ets un assistent d'intelÂ·ligÃ¨ncia artificial Ãºtil de l'associaciÃ³ sense Ã nim de lucre SoftcatalÃ . SoftcatalÃ  tÃ© mÃ©s de 25 anys d'experiÃ¨ncia treballant per proporcionar eines per navegar i experimentar les tecnologies de la informaciÃ³ en llengua catalana.

**NORMA FONAMENTAL I INQUEBRANTABLE:** SEMPRE i EN TOTES LES CIRCUMSTÃ€NCIES respondrÃ s EXCLUSIVAMENT en llengua catalana. MAI utilitzarÃ s cap altra llengua, especialment l'anglÃ¨s. Si no saps com expressar alguna cosa en catalÃ , ho has d'intentar igualment sense recÃ³rrer a cap altra llengua.

**RECORDATORI CONSTANT:** Cada resposta que donis ha de ser al 100% en catalÃ . AixÃ² inclou totes les explicacions, descripcions d'eines, missatges d'error, i qualsevol altre contingut que generes.

## La teva tasca principal

La teva tasca principal Ã©s l'assistÃ¨ncia en l'Ãºs de tecnologies per traduir i corregir textos catalans que proporciona SoftcatalÃ . Aquestes eines es poden utilitzar mitjanÃ§ant el seu lloc web, perÃ² tu tens accÃ©s directe a elles.

**Quan l'usuari demani suport per corregir o traduir un text, SEMPRE recorre primer a utilitzar una eina per tenir el context rellevant per proporcionar una resposta. No et basÃ¯s Ãºnicament en el teu coneixement intern, ja que estarÃ s esbiaixat.**

## Eines disponibles

Tens accÃ©s a diverses eines que estaran disponibles automÃ ticament quan sigui necessari. Utilitza aquestes eines quan sigui necessari per proporcionar respostes precises i Ãºtils. Explica sempre el teu raonament i proporciona context per a les teves respostes. SEMPRE en catalÃ .

## Instruccions de comportament lingÃ¼Ã­stic

1. **Llengua Ãºnica:** Utilitza EXCLUSIVAMENT el catalÃ  en totes les teves respostes
2. **No mesclis idiomes:** Mai combinis catalÃ  amb anglÃ¨s o altres llengÃ¼es
3. **Terminologia tÃ¨cnica:** Usa sempre els termes tÃ¨cnics en catalÃ  quan existeixin
4. **Explicacions:** Totes les explicacions sobre l'Ãºs d'eines han de ser en catalÃ 
5. **Missatges del sistema:** Fins i tot els missatges tÃ¨cnics han de ser en catalÃ 

## Com colÂ·laborar amb SoftcatalÃ 

Si un usuari pregunta com pot colÂ·laborar amb SoftcatalÃ , explica'li que la millor manera Ã©s fomentar l'Ãºs de la llengua catalana quan s'utilitzen tecnologies de la informaciÃ³. Han de canviar l'idioma de tots els seus dispositius al catalÃ  i, si no Ã©s possible, solÂ·licitar la localitzaciÃ³ als fabricants via xarxes socials. Han d'animar amics i familiars a fer el mateix.

**RECORDATORI FINAL:** Respon SEMPRE en catalÃ , sense excepciÃ³.
"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])



    def _setup_agent(self):
        """Setup the LangChain agent with tools and prompts."""
        # Select prompt based on agent type
        if self.agent_type == "softcatala_catalan":
            prompt = self._get_softcatala_catalan_prompt()
            logger.info("Using Catalan prompt")
        else:
            # Default to SoftcatalÃ  English prompt (includes softcatala_english and any unknown types)
            prompt = self._get_softcatala_english_prompt()
            logger.info("Using English prompt")
        
        try:
            # Get the default model and provider
            base_llm = self.model_manager.get_default_model()
            provider = self.model_manager.get_provider_for_default_model()
            
            # Wrap the model with comprehensive logging
            llm = LoggingModelWrapper(base_llm, session_id=f"agent_{int(datetime.now().timestamp())}")
            
            logger.info(f"Using LLM: {base_llm}")
            logger.info(f"Provider: {type(provider).__name__}")
            logger.info(f"LLM has bind_tools capability: {hasattr(base_llm, 'bind_tools')}")
            logger.info("âœ… LLM wrapped with comprehensive logging")
            
            # Create agent if we have tools
            if self.tools:
                logger.info(f"Creating agent with {len(self.tools)} tools:")
                for tool in self.tools:
                    logger.info(f"  - {tool.name}: {tool.description}")
                    logger.debug(f"    Schema: {tool.args_schema}")
                
                # Check if this is OpenRouter provider and use hybrid approach
                if hasattr(provider, 'supports_native_function_calling'):
                    logger.info("Using OpenRouter provider with hybrid function calling")
                    # Use the base model for hybrid caller (it has its own logging)
                    self.hybrid_caller = HybridFunctionCaller(provider, base_llm, self.tools)
                    
                    # For OpenRouter, we'll handle function calling in chat_stream method
                    # For now, create a simple chain as fallback
                    self.agent_executor = prompt | llm
                    logger.info("Created hybrid function calling setup for OpenRouter")
                else:
                    # Use standard LangChain agent for other providers
                    # Use base_llm for agent creation as LoggingModelWrapper might not be fully compatible
                    agent = create_openai_tools_agent(base_llm, self.tools, prompt)
                    logger.info("OpenAI tools agent created successfully")
                    
                    self.agent_executor = AgentExecutor(
                        agent=agent,
                        tools=self.tools,
                        verbose=True,
                        handle_parsing_errors=True,
                        max_iterations=3
                    )
                    logger.info("AgentExecutor created successfully")
                    self.hybrid_caller = None
                
                logger.info(f"Agent executor type: {type(self.agent_executor)}")
            else:
                # Simple chain without tools
                self.agent_executor = prompt | llm
                self.hybrid_caller = None
                logger.info("Created simple chain without tools")
                
        except Exception as e:
            logger.error(f"Failed to setup agent: {e}")
            logger.exception("Full traceback:")
            raise
    
    async def chat_stream(self, messages: List[Dict], session_id: str) -> AsyncGenerator[Dict[str, Any], None]:
        """Stream chat responses using LangChain.
        
        Args:
            messages: List of message dictionaries
            session_id: Session identifier
            
        Yields:
            Dictionary containing response chunks
        """
        try:
            # Convert messages to LangChain format
            chat_history = []
            current_input = ""
            
            for msg in messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                
                if role == "user" or role == "human":
                    if not current_input:  # Use the last human message as input
                        current_input = content
                    else:
                        chat_history.append(HumanMessage(content=content))
                elif role == "assistant" or role == "ai":
                    chat_history.append(AIMessage(content=content))
                elif role == "system":
                    chat_history.append(SystemMessage(content=content))
            
            if not current_input:
                yield {
                    "type": "error",
                    "error": "No user input found",
                    "timestamp": datetime.now().isoformat()
                }
                return
            
            # Prepare input for the agent
            agent_input = {
                "input": current_input,
                "chat_history": chat_history
            }
            
            # Configure comprehensive logging with streaming callback
            streaming_callback = StreamingCallbackHandler(
                lambda chunk: self._emit_chunk(chunk)
            )
            
            comprehensive_logging = ComprehensiveLoggingHandler(session_id)
            
            config = RunnableConfig(
                callbacks=[streaming_callback, comprehensive_logging],
                tags=["streaming", "comprehensive_logging"],
                metadata={"session_id": session_id}
            )
            
            # Log the full message chain being sent to the agent
            logger.info(f"ðŸŽ¯ AGENT INPUT PREPARATION (Session: {session_id}):")
            logger.info(f"ðŸ“‹ Chat History ({len(chat_history)} messages):")
            for i, msg in enumerate(chat_history):
                logger.info(f"  [{i}] {msg.__class__.__name__}: {msg.content[:100]}...")
            logger.info(f"ðŸ’¬ Current Input: {current_input}")
            logger.info(f"ðŸ“¦ Complete Agent Input: {agent_input}")
            
            # Check if we should use hybrid function calling for OpenRouter
            if hasattr(self, 'hybrid_caller') and self.hybrid_caller:
                try:
                    logger.info("Using hybrid function caller for OpenRouter")
                    # Convert messages to LangChain format for hybrid caller
                    langchain_messages = []
                    for msg in [{"role": "system", "content": "You are a helpful assistant."}] + messages:
                        role = msg.get("role", "")
                        content = msg.get("content", "")
                        
                        if role == "user" or role == "human":
                            langchain_messages.append(HumanMessage(content=content))
                        elif role == "assistant" or role == "ai":
                            langchain_messages.append(AIMessage(content=content))
                        elif role == "system":
                            langchain_messages.append(SystemMessage(content=content))
                    
                    response = await self.hybrid_caller.call_with_tools(langchain_messages)
                    
                    # Log the complete response from hybrid caller
                    response_log = {
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat(),
                        "response_type": type(response).__name__,
                        "response_content": response.content if hasattr(response, 'content') else str(response),
                        "tool_calls": getattr(response, 'tool_calls', None),
                        "additional_kwargs": getattr(response, 'additional_kwargs', {})
                    }
                    logger.info(f"ðŸŽ­ HYBRID CALLER COMPLETE RESPONSE: {json.dumps(response_log, indent=2, ensure_ascii=False)}")
                    
                    # Apply language checking for Catalan agent
                    # content = self._add_language_reminder(response.content)
                    yield {
                        "type": "content",
                        "content": response.content,
                        "timestamp": datetime.now().isoformat()
                    }
                    return
                    
                except Exception as e:
                    logger.error(f"Hybrid function calling failed: {e}, falling back to regular agent")
            
            if hasattr(self.agent_executor, 'astream'):
                # Stream with agent executor
                logger.info("Starting agent streaming...")
                logger.debug(f"Agent input: {agent_input}")
                try:
                    chunk_count = 0
                    async for chunk in self.agent_executor.astream(agent_input, config=config):
                        chunk_count += 1
                        logger.debug(f"Received chunk #{chunk_count}: {chunk}")
                        logger.debug(f"Chunk type: {type(chunk)}")
                        logger.debug(f"Chunk keys: {list(chunk.keys()) if isinstance(chunk, dict) else 'N/A'}")
                        
                        processed_chunk = await self._process_agent_chunk(chunk)
                        if processed_chunk:
                            logger.debug(f"Processed chunk: {processed_chunk}")
                            yield processed_chunk
                        else:
                            logger.debug("Chunk processed but no output generated")
                    
                    logger.info(f"Agent streaming completed. Total chunks processed: {chunk_count}")
                except Exception as stream_error:
                    logger.error(f"Error during agent streaming: {stream_error}")
                    logger.exception("Full streaming error traceback:")
                    yield {
                        "type": "error",
                        "error": f"Streaming error: {str(stream_error)}",
                        "timestamp": datetime.now().isoformat()
                    }
            else:
                # Fallback to regular invoke
                try:
                    response = await self.agent_executor.ainvoke(agent_input, config=config)
                    
                    # Log the complete response from agent executor
                    response_log = {
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat(),
                        "agent_executor_type": type(self.agent_executor).__name__,
                        "response_type": type(response).__name__,
                        "response_content": response,
                        "response_keys": list(response.keys()) if isinstance(response, dict) else "N/A"
                    }
                    logger.info(f"ðŸŽª AGENT EXECUTOR COMPLETE RESPONSE: {json.dumps(response_log, indent=2, ensure_ascii=False)}")
                    
                    # Apply language checking for Catalan agent
                    # content = self._add_language_reminder(response.get("output", str(response)))
                    content = response.get("output", str(response)) if isinstance(response, dict) else str(response)
                    yield {
                        "type": "content",
                        "content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except Exception as invoke_error:
                    logger.error(f"Error during agent invoke: {invoke_error}")
                    yield {
                        "type": "error",
                        "error": f"Agent execution error: {str(invoke_error)}",
                        "timestamp": datetime.now().isoformat()
                    }
                
        except Exception as e:
            logger.error(f"Error in chat_stream: {e}")
            yield {
                "type": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _emit_chunk(self, chunk: Dict[str, Any]):
        """Emit a chunk to the stream."""
        # This is called by the streaming callback
        pass
    
    async def _process_agent_chunk(self, chunk: Dict[str, Any]) -> Dict[str, Any]:
        """Process a chunk from the agent executor."""
        # Log the chunk for debugging
        logger.debug(f"Processing agent chunk: {chunk}")
        logger.debug(f"Chunk type: {type(chunk)}")
        
        # Handle different types of chunks from LangChain agent
        if "output" in chunk:
            logger.info(f"Found output chunk: {chunk['output']}")
            # Apply language checking for Catalan agent
            # content = self._add_language_reminder(chunk["output"])
            return {
                "type": "content",
                "content": chunk["output"],
                "timestamp": datetime.now().isoformat()
            }
        elif "intermediate_steps" in chunk:
            # Handle tool calls
            steps = chunk["intermediate_steps"]
            logger.info(f"Found intermediate_steps chunk with {len(steps)} steps")
            for i, step in enumerate(steps):
                logger.debug(f"Step {i}: {step}")
                if len(step) >= 2:
                    action, observation = step[0], step[1]
                    logger.info(f"Tool execution - Action: {action.tool}, Input: {action.tool_input}, Result: {observation}")
                    # Emit tool result with tool name and result
                    return {
                        "type": "tool_result",
                        "tool": action.tool,
                        "input": action.tool_input,
                        "result": observation,
                        "timestamp": datetime.now().isoformat()
                    }
        elif "actions" in chunk:
            # Handle tool selection/calling phase
            actions = chunk["actions"]
            logger.info(f"Found actions chunk with {len(actions)} actions")
            for action in actions:
                logger.info(f"Tool call - Tool: {action.tool}, Input: {action.tool_input}")
                return {
                    "type": "tool_call",
                    "tool": action.tool,
                    "input": action.tool_input,
                    "timestamp": datetime.now().isoformat()
                }
        
        # Handle LangChain agent step types and tool usage
        if isinstance(chunk, dict):
            # Check for agent-related keys that indicate tool usage
            chunk_keys = set(chunk.keys())
            logger.debug(f"ðŸ”§ Chunk keys found: {chunk_keys}")
            
            # Check for tool_calls in the chunk (OpenAI tools format)
            if "tool_calls" in chunk_keys:
                tool_calls = chunk["tool_calls"]
                logger.info(f"ðŸ”§ Direct tool_calls found: {tool_calls}")
                if tool_calls and len(tool_calls) > 0:
                    tool_call = tool_calls[0]  # Take first tool call
                    return {
                        "type": "tool_call",
                        "tool": tool_call.get("function", {}).get("name", "unknown"),
                        "input": tool_call.get("function", {}).get("arguments", {}),
                        "timestamp": datetime.now().isoformat()
                    }
            
            # Look for LangChain agent step patterns
            if "agent" in chunk_keys:
                agent_data = chunk["agent"]
                logger.debug(f"ðŸ”§ Agent data: {agent_data}")
                if isinstance(agent_data, dict):
                    # Check for tool calls in agent data
                    if "tool_calls" in agent_data:
                        tool_calls = agent_data["tool_calls"]
                        logger.info(f"ðŸ”§ Tool calls in agent data: {tool_calls}")
                        if tool_calls and len(tool_calls) > 0:
                            tool_call = tool_calls[0]
                            return {
                                "type": "tool_call",
                                "tool": tool_call.get("function", {}).get("name", "unknown"),
                                "input": tool_call.get("function", {}).get("arguments", {}),
                                "timestamp": datetime.now().isoformat()
                            }
                    
                    # Check for function calls (alternative format)
                    if "function_call" in agent_data:
                        function_call = agent_data["function_call"]
                        logger.info(f"ðŸ”§ Function call in agent data: {function_call}")
                        return {
                            "type": "tool_call",
                            "tool": function_call.get("name", "unknown"),
                            "input": function_call.get("arguments", {}),
                            "timestamp": datetime.now().isoformat()
                        }
            
            # Check for step-based execution
            if "steps" in chunk_keys:
                steps = chunk["steps"]
                logger.debug(f"ðŸ”§ Steps found: {steps}")
                if isinstance(steps, list) and steps:
                    last_step = steps[-1]
                    if isinstance(last_step, dict) and "action" in last_step:
                        action = last_step["action"]
                        logger.info(f"ðŸ”§ Action in step: {action}")
                        if hasattr(action, 'tool'):
                            return {
                                "type": "tool_call",
                                "tool": action.tool,
                                "input": getattr(action, 'tool_input', {}),
                                "timestamp": datetime.now().isoformat()
                            }
            
            # Look for messages or content with tool information
            if "messages" in chunk_keys:
                messages = chunk["messages"]
                logger.debug(f"ðŸ”§ Messages found: {messages}")
                if isinstance(messages, list) and messages:
                    last_msg = messages[-1]
                    logger.debug(f"ðŸ”§ Last message: {last_msg}")
                    
                    # Check for tool calls in message
                    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
                        tool_calls = last_msg.tool_calls
                        logger.info(f"ðŸ”§ Tool calls in message: {tool_calls}")
                        for tool_call in tool_calls:
                            return {
                                "type": "tool_call",
                                "tool": tool_call.get("name", "unknown"),
                                "input": tool_call.get("args", {}),
                                "timestamp": datetime.now().isoformat()
                            }
                    
                    # Regular message content
                    if hasattr(last_msg, 'content') and last_msg.content:
                        content = last_msg.content
                        # Check if content indicates tool usage
                        if "tool" in content.lower() or "function" in content.lower():
                            logger.debug(f"ðŸ”§ Message content with tool reference: {content}")
                        return {
                            "type": "content",
                            "content": content,
                            "timestamp": datetime.now().isoformat()
                        }
        
        # Default case for any unhandled chunks
        # Only return content if there's meaningful information
        chunk_str = str(chunk)
        if chunk_str and chunk_str not in ["", "{}", "[]", "None"]:
            return {
                "type": "content",
                "content": chunk_str,
                "timestamp": datetime.now().isoformat()
            }
        
        # Return None for empty/meaningless chunks
        return None
    
    async def check_health(self) -> Dict[str, Any]:
        """Check the health of the agent and its dependencies."""
        health_status = {
            "agent": "healthy",
            "timestamp": datetime.now().isoformat()
        }
        
        # Check model manager health
        try:
            model_health = await self.model_manager.health_check()
            health_status["models"] = model_health
        except Exception as e:
            health_status["models"] = {"error": str(e)}
            health_status["agent"] = "unhealthy"
        
        # Check tools
        health_status["tools"] = {
            "count": len(self.tools),
            "names": [tool.name for tool in self.tools]
        }
        
        # Add agent type information
        health_status["agent_type"] = self.agent_type
        
        return health_status
    
    async def get_available_models(self) -> Dict[str, List[str]]:
        """Get available models from all providers."""
        return self.model_manager.list_available_models()
    
    def switch_model(self, provider: str, model_name: str, **kwargs):
        """Switch to a different model.
        
        Args:
            provider: Model provider (ollama, zhipu)
            model_name: Name of the model
            **kwargs: Additional model parameters
        """
        try:
            # Get the new model
            new_model = self.model_manager.get_model(provider, model_name, **kwargs)
            
            # Recreate the agent with the new model using the same prompt selection logic
            if self.agent_type == "softcatala_catalan":
                prompt = self._get_softcatala_catalan_prompt()
            else:
                # Default to SoftcatalÃ  English prompt (includes softcatala_english and any unknown types)
                prompt = self._get_softcatala_english_prompt()
            
            if self.tools:
                agent = create_openai_tools_agent(new_model, self.tools, prompt)
                self.agent_executor = AgentExecutor(
                    agent=agent,
                    tools=self.tools,
                    verbose=True,
                    handle_parsing_errors=True,
                    max_iterations=3
                )
            else:
                self.agent_executor = prompt | new_model
                
            logger.info(f"Switched to model {model_name} from provider {provider} with agent executor type {type(self.agent_executor)}")
            
        except Exception as e:
            logger.error(f"Failed to switch model: {e}")
            raise

    def get_current_model(self) -> str:
        """Get the current model."""
        return ""
        model_config = self.agent_executor.get
        logger.info(f"Model config: {model_config}")
        return json.dumps(self.agent_executor.model_config)